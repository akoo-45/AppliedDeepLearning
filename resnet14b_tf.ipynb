{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "resnet14b_tf.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akoo-45/AppliedDeepLearning/blob/master/resnet14b_tf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ql44ZDpkH6Z",
        "colab_type": "text"
      },
      "source": [
        "This notebook runs a ResNet14b convolutional neural networks for classifying 5 particle images in a simulated LArTPC detector available from the [public dataset](http://deeplearnphysics.org/DataChallenge/). We use Keras to train the network and larcv_threadio to fetch data from larcv files.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "ssh hopper\n",
        "cd /data/ashley.koo/larcv-tutorial\n",
        "# Download necessary libraries (see appendix in [writeup]\n",
        "(https://docs.google.com/document/d/1jElkhcZG15OG6Azza3dgda2aagX1vHUS5YlMw8LcOBc/edit)) \n",
        "python akoo_resnet14b_tf.py\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_u5rIVdFRiJ",
        "colab_type": "text"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eq_8A5TFyHt_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from larcv import larcv\n",
        "from larcv.dataloader2 import larcv_threadio\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os,sys,time\n",
        "import six\n",
        "import tensorflow as tf\n",
        "import tensorflow.contrib.keras as keras \n",
        "\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers import (\n",
        "    Input,\n",
        "    Activation,\n",
        "    Dense,\n",
        "    Flatten,\n",
        "\t   Dropout \n",
        ")\n",
        "from keras.layers.convolutional import (\n",
        "    Conv2D,\n",
        "    MaxPooling2D,\n",
        "    AveragePooling2D\n",
        ")\n",
        "from keras.layers.merge import Add\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.regularizers import l2\n",
        "from keras import backend as K"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTFgDc8UFZny",
        "colab_type": "text"
      },
      "source": [
        "# Configurations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AS3eXYdhFWXo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tensorflow/gpu start-up configuration\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "os.environ['CUDA_DEVICE_ORDER']='PCI_BUS_ID'\n",
        "os.environ['CUDA_VISIBLE_DEVICES']='2'\n",
        "\n",
        "# Making directory paths\n",
        "TUTORIAL_DIR     = '.'\n",
        "TRAIN_IO_CONFIG  = os.path.join(TUTORIAL_DIR, 'tf/io_train.cfg') # configuration file for train_io stored in './tf/io_train.cfg'\n",
        "TEST_IO_CONFIG   = os.path.join(TUTORIAL_DIR, 'tf/io_test.cfg' ) # configuration file for test_io\n",
        "TRAIN_BATCH_SIZE = 10 # Unused variable\n",
        "TEST_BATCH_SIZE  = 100 # Unused variable \n",
        "LOGDIR           = 'resnet_log' \n",
        "ITERATIONS       = 10 # Unused variable\n",
        "\n",
        "# Check that the log directory is empty \n",
        "train_logdir = os.path.join(LOGDIR,'train')\n",
        "test_logdir  = os.path.join(LOGDIR,'test')\n",
        "\n",
        "if not os.path.isdir(train_logdir): os.makedirs(train_logdir)\n",
        "if not os.path.isdir(test_logdir):  os.makedirs(test_logdir)\n",
        "if len(os.listdir(train_logdir)) or len(os.listdir(test_logdir)):\n",
        "  sys.stderr.write('Error: train or test log dir not empty...\\n')\n",
        "  raise OSError"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GBBRCd8FnWh",
        "colab_type": "text"
      },
      "source": [
        "# Configuring Data Reader \n",
        "We prepare two data reader instances: one for training and another for testing the network. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DqQG4Oonv1a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "# Step 0: Configure the IO readers\n",
        "#\n",
        "\n",
        "# Training dataset \n",
        "train_io = larcv_threadio()  \n",
        "train_io_cfg = {'filler_name' : 'TrainIO',\n",
        "                'verbosity'   : 10,\n",
        "                'filler_cfg'  : TRAIN_IO_CONFIG}\n",
        "train_io.configure(train_io_cfg)   \n",
        "train_io.start_manager(TRAIN_BATCH_SIZE) # arg: # of images to store\n",
        "time.sleep(2)\n",
        "train_io.next() # transforms data into numpy array\n",
        "\n",
        "# Test dataset \n",
        "test_io = larcv_threadio()   # create io interface\n",
        "test_io_cfg = {'filler_name' : 'TestIO',\n",
        "               'verbosity'   : 10,\n",
        "               'filler_cfg'  : TEST_IO_CONFIG}\n",
        "test_io.configure(test_io_cfg)   # configure\n",
        "test_io.start_manager(TEST_BATCH_SIZE) # start read thread\n",
        "time.sleep(2) \n",
        "test_io.next()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCQW7FRPz4XI",
        "colab_type": "text"
      },
      "source": [
        "#Defining a network\n",
        "We use 4 resnet blocks for ResNet14b. \n",
        "\n",
        "Referenced Kaiming He's repository, found here: \n",
        "https://github.com/KaimingHe/deep-residual-networks\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvV35BmJnQOX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# \n",
        "# Step 1: Define ResNet14b Network\n",
        "#\n",
        "def _bn_relu(input):\n",
        "    \"\"\"Helper to build a BN (batch normalization) -> relu block\n",
        "    \"\"\"\n",
        "    norm = BatchNormalization(axis=CHANNEL_AXIS)(input)\n",
        "    return Activation(\"relu\")(norm)\n",
        "\n",
        "def _conv_bn_relu(**conv_params):\n",
        "    \"\"\"Helper to build a conv -> BN -> relu block\n",
        "    \"\"\"\n",
        "    filters = conv_params[\"filters\"]\n",
        "    kernel_size = conv_params[\"kernel_size\"]\n",
        "    strides = conv_params.setdefault(\"strides\", (1, 1)) # Overrides strides?\n",
        "    kernel_initializer = conv_params.setdefault(\"kernel_initializer\", \"he_normal\")\n",
        "    padding = conv_params.setdefault(\"padding\", \"same\") # Padding, SAME????????? TODO\n",
        "    kernel_regularizer = conv_params.setdefault(\"kernel_regularizer\", l2(1.e-4))\n",
        "\n",
        "    def f(input):\n",
        "        conv = Conv2D(filters=filters, kernel_size=kernel_size,\n",
        "                      strides=strides, padding=padding,\n",
        "                      kernel_initializer=kernel_initializer,\n",
        "                      kernel_regularizer=kernel_regularizer)(input)\n",
        "        return _bn_relu(conv)\n",
        "\n",
        "    return f\n",
        "\n",
        "# TODO: reduce code later by duplicating with _conv_bn_relu\n",
        "def _conv_bn(**conv_params):\n",
        "    \"\"\"Helper to build a conv -> BN -> relu block\n",
        "    \"\"\"\n",
        "    filters = conv_params[\"filters\"]\n",
        "    kernel_size = conv_params[\"kernel_size\"]\n",
        "    strides = conv_params.setdefault(\"strides\", (1, 1)) # Overrides strides?\n",
        "    kernel_initializer = conv_params.setdefault(\"kernel_initializer\", \"he_normal\")\n",
        "    padding = conv_params.setdefault(\"padding\", \"same\") # Padding, SAME?????????\n",
        "    kernel_regularizer = conv_params.setdefault(\"kernel_regularizer\", l2(1.e-4))\n",
        "\n",
        "    def f(input):\n",
        "        conv = Conv2D(filters=filters, kernel_size=kernel_size,\n",
        "                      strides=strides, padding=padding,\n",
        "                      kernel_initializer=kernel_initializer,\n",
        "                      kernel_regularizer=kernel_regularizer)(input)\n",
        "        return BatchNormalization(axis=CHANNEL_AXIS)(conv)\n",
        "\n",
        "    return f\n",
        "\n",
        "\n",
        "def _bn_relu_conv(**conv_params):\n",
        "    \"\"\"Helper to build a BN -> relu -> conv block.\n",
        "    This is an improved scheme proposed in http://arxiv.org/pdf/1603.05027v2.pdf\n",
        "    \"\"\"\n",
        "    filters = conv_params[\"filters\"]\n",
        "    kernel_size = conv_params[\"kernel_size\"]\n",
        "    strides = conv_params.setdefault(\"strides\", (1, 1))\n",
        "    kernel_initializer = conv_params.setdefault(\"kernel_initializer\", \"he_normal\")\n",
        "    padding = conv_params.setdefault(\"padding\", \"same\")\n",
        "    kernel_regularizer = conv_params.setdefault(\"kernel_regularizer\", l2(1.e-4))\n",
        "\n",
        "    def f(input):\n",
        "        activation = _bn_relu(input)\n",
        "        return Conv2D(filters=filters, kernel_size=kernel_size,\n",
        "                      strides=strides, padding=padding,\n",
        "                      kernel_initializer=kernel_initializer,\n",
        "                      kernel_regularizer=kernel_regularizer)(activation)\n",
        "\n",
        "    return f\n",
        "\n",
        "def _shortcut(input, residual):\n",
        "    \"\"\"Adds a shortcut between input (pool) and residual block and merges them with \"sum\"\n",
        "    \"\"\"\n",
        "    # Expand channels of shortcut to match residual.\n",
        "    # Stride appropriately to match residual (width, height)\n",
        "    # Should be int if network architecture is correctly configured.\n",
        "    print(\"Entering shortcut\")\n",
        "    input_shape = K.int_shape(input)\n",
        "    residual_shape = K.int_shape(residual)\n",
        "    print (input_shape)\n",
        "    print (residual_shape) \n",
        "    print (\"Input channels \", input_shape[CHANNEL_AXIS])\n",
        "    print (\"Residual channels \", residual_shape[CHANNEL_AXIS])\n",
        "    stride_width = int(round(input_shape[ROW_AXIS] / residual_shape[ROW_AXIS]))\n",
        "    stride_height = int(round(input_shape[COL_AXIS] / residual_shape[COL_AXIS]))\n",
        "    equal_channels = input_shape[CHANNEL_AXIS] == residual_shape[CHANNEL_AXIS]\n",
        "\n",
        "    shortcut = input\n",
        "    # 1 X 1 conv if shape is different. Else identity.\n",
        "    if stride_width > 1 or stride_height > 1 or not equal_channels:\n",
        "        print(\"The number of filters \", residual_shape[CHANNEL_AXIS]) # TODO\n",
        "        shortcut = Conv2D(filters=residual_shape[CHANNEL_AXIS],\n",
        "                          kernel_size=(1, 1),\n",
        "                          strides=(stride_width, stride_height),\n",
        "                          padding=\"valid\",\n",
        "                          kernel_initializer=\"he_normal\",\n",
        "                          kernel_regularizer=l2(0.0001))(input)\n",
        "\n",
        "    return add([shortcut, residual])\n",
        "\n",
        "def _residual_block(block_function, filters, repetitions, is_first_layer=False):\n",
        "    \"\"\"Builds a residual block with repeating bottleneck blocks.\n",
        "    \"\"\"\n",
        "    def f(input): \n",
        "\t      print( \"Entering residual block\" ) \n",
        "        for i in range(repetitions):\n",
        "            init_strides = (1, 1) \n",
        "            if i == 0 and not is_first_layer: # NOTE: this was confusing to interpret\n",
        "                init_strides = (2, 2) # Why strides = (2,2) for the rest of the layers?\n",
        "            input = block_function(filters=filters, init_strides=init_strides,  \n",
        "                                   is_first_block_of_first_layer=(is_first_layer and i == 0))(input)\n",
        "        return input\n",
        "\n",
        "    return f\n",
        "\n",
        "def basic_block(filters, init_strides=(1, 1), is_first_block_of_first_layer=False):\n",
        "    \"\"\"Basic 3 X 3 convolution blocks for use on resnets with layers <= 34.\n",
        "    Follows improved proposed scheme in http://arxiv.org/pdf/1603.05027v2.pdf\n",
        "    \"\"\"\n",
        "    def f(input):\n",
        "\n",
        "        if is_first_block_of_first_layer:\n",
        "            # don't repeat bn->relu since we just did bn->relu->maxpool\n",
        "            conv1 = Conv2D(filters=filters, kernel_size=(3, 3),\n",
        "                           strides=init_strides,\n",
        "                           padding=\"same\",\n",
        "                           kernel_initializer=\"he_normal\",\n",
        "                           kernel_regularizer=l2(1e-4))(input)\n",
        "        else:\n",
        "            conv1 = _bn_relu_conv(filters=filters, kernel_size=(3, 3),\n",
        "                                  strides=init_strides)(input)\n",
        "\n",
        "        residual = _bn_relu_conv(filters=filters, kernel_size=(3, 3))(conv1)\n",
        "        return _shortcut(input, residual)\n",
        "\n",
        "    return f\n",
        "\n",
        "\n",
        "def bottleneck(filters, init_strides=(1, 1), is_first_block_of_first_layer=False):\n",
        "    \"\"\"Bottleneck architecture for > 34 layer resnet.\n",
        "    Follows improved proposed scheme in http://arxiv.org/pdf/1603.05027v2.pdf\n",
        "    Returns:\n",
        "        A final conv layer of filters * 4\n",
        "    \"\"\"\n",
        "    def f(input):\n",
        "        # if is_first_block_of_first_layer:\n",
        "            # don't repeat bn->relu since we just did bn->relu->maxpool # you still did it... (possible optimization) TODO **\n",
        "        #     conv_1_1 = Conv2D(filters=filters, kernel_size=(1, 1),\n",
        "        #                       strides=init_strides,\n",
        "        #                       padding=\"same\",\n",
        "        #                       kernel_initializer=\"he_normal\",\n",
        "        #                       kernel_regularizer=l2(1e-4))(input)\n",
        "        # else:\n",
        "            # revert later ** changed all 3  _bn_relu_conv --> _conv_bn_relu (possible optimization) TODO\n",
        "            # see if padding needs to change TODO padding = same for ALL \n",
        "        print(\"Entering bottleneck stage\")\n",
        "\t      print(\"Filters\", filters, \"strides\", init_strides,\"before conv_bn_relu\")\n",
        "        conv_1_1 = _conv_bn_relu(filters=filters, kernel_size=(1, 1),\n",
        "                                     strides=init_strides)(input) # Pass on strides!?\n",
        "        conv_3_3 = _conv_bn_relu(filters=filters, kernel_size=(3, 3))(conv_1_1)\n",
        "        # filters = filters * 4 is true if you check the ethereon model for Resnet 50 \n",
        "        # might have to edit ** last layer doesn't have relu...  ** TODO\n",
        "        # residual = _conv_bn_relu(filters=filters * 4, kernel_size=(1, 1))(conv_3_3) # THIS HAS DIFFERENT PADDING THOUGH ?? Does padding = same do the trick??\n",
        "        residual = _conv_bn(filters=filters * 4, kernel_size=(1, 1))(conv_3_3)\n",
        "        print(\"Filter used for the residual layer \",  filters * 4)\n",
        "        return _shortcut(input, residual)\n",
        "\n",
        "    return f\n",
        "\n",
        "def _handle_dim_ordering():\n",
        "    global ROW_AXIS\n",
        "    global COL_AXIS\n",
        "    global CHANNEL_AXIS\n",
        "\n",
        "\t  # If K backend engine is tf\n",
        "\t  CHANNEL_AXIS = 1\n",
        "\t  ROW_AXIS = 2\n",
        "\t  COL_AXIS = 3\n",
        "\n",
        "def _get_block(identifier):\n",
        "    if isinstance(identifier, six.string_types):\n",
        "        res = globals().get(identifier)\n",
        "        if not res:\n",
        "            raise ValueError('Invalid {}'.format(identifier))\n",
        "        return res\n",
        "    return identifier\n",
        "\n",
        "\n",
        "class ResnetBuilder(object):\n",
        "    @staticmethod\n",
        "    def build(input_shape, num_outputs, block_fn, repetitions):\n",
        "        \"\"\"Builds a custom ResNet like architecture.\n",
        "        Args:\n",
        "            input_shape: The input shape in the form (nb_channels, nb_rows, nb_cols)\n",
        "            num_outputs: The number of outputs at final softmax layer\n",
        "            block_fn: The block function to use. This is either `basic_block` or `bottleneck`.\n",
        "                The original paper used basic_block for layers < 50\n",
        "            repetitions: Number of repetitions of various block units.\n",
        "                At each block unit, the number of filters are doubled and the input size is halved\n",
        "        Returns:\n",
        "            The keras `Model`.\n",
        "        \"\"\"\n",
        "        _handle_dim_ordering()\n",
        "        if len(input_shape) != 3:\n",
        "            raise Exception(\"Input shape should be a tuple (nb_channels, nb_rows, nb_cols)\")\n",
        "\n",
        "        # Permute dimension order if necessary *\n",
        "        # if K.image_data_format() == 'tf':\n",
        "        #     input_shape = (input_shape[1], input_shape[2], input_shape[0]) # rows, cols, samples? *?\n",
        "\n",
        "        # Load function from str if needed.\n",
        "        block_fn = _get_block(block_fn)\n",
        "\n",
        "        input = Input(shape=input_shape) # creation of keras tensor  \n",
        "        conv1 = _conv_bn_relu(filters=64, kernel_size=(7, 7), strides=(2, 2))(input) # padding = 3? *? TODO\n",
        "        pool1 = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding=\"same\")(conv1)\n",
        "\n",
        "        block = pool1\n",
        "        filters = 64\n",
        "        for i, r in enumerate(repetitions): # handy-dandy!\n",
        "            block = _residual_block(block_fn, filters=filters, repetitions=r, is_first_layer=(i == 0))(block) # block is input for bottleneck layer \n",
        "            filters *= 2\n",
        "\t      block = _bn_relu(block)\n",
        "\n",
        "        # Last activation\n",
        "        # I think this is just _relu... for the last block  but will leave it TODO\n",
        "        block = _bn_relu(block) \n",
        "\n",
        "        # Classifier block\n",
        "        # changed from strides=(1,1) to (18,18) ** TODO don't know why kernel size doesn't show up in Kazu's... \n",
        "        block_shape = K.int_shape(block)\n",
        "        pool2 = AveragePooling2D(pool_size=(block_shape[ROW_AXIS], block_shape[COL_AXIS]),\n",
        "                                 strides=(18, 18))(block) # where is the kernel?? 7?? TODO\n",
        "\t      flatten1 = Flatten()(pool2)  \n",
        "\t      dropout = Dropout(rate=0.2)(flatten1) \n",
        "\t      dense = Dense(units=num_outputs, kernel_initializer=\"he_normal\",\n",
        "                      activation=\"softmax\")(dropout) \n",
        "\n",
        "        model = Model(inputs=input, outputs=dense)\n",
        "\t      print(\"returning model\")\n",
        "        return model\n",
        "\n",
        "    \n",
        "    # Custom method for ResNet14b \n",
        "    @staticmethod\n",
        "    def build_resnet_14b(input_shape, num_outputs):\n",
        "        return ResnetBuilder.build(input_shape, num_outputs, bottleneck, [1, 1, 1, 1])\n",
        "\n",
        "    # @staticmethod\n",
        "    # def build_resnet_18(input_shape, num_outputs):\n",
        "    #     return ResnetBuilder.build(input_shape, num_outputs, basic_block, [2, 2, 2, 2])\n",
        "\n",
        "    # **\n",
        "    # @staticmethod\n",
        "    # def build_resnet_34(input_shape, num_outputs):\n",
        "    #     return ResnetBuilder.build(input_shape, num_outputs, basic_block, [3, 4, 6, 3])\n",
        "\n",
        "    #     @staticmethod\n",
        "    #     def build_resnet_50(input_shape, num_outputs):\n",
        "    #         return ResnetBuilder.build(input_shape, num_outputs, bottleneck, [3, 4, 6, 3])\n",
        "\n",
        "    # @staticmethod\n",
        "    # def build_resnet_101(input_shape, num_outputs):\n",
        "    #     return ResnetBuilder.build(input_shape, num_outputs, bottleneck, [3, 4, 23, 3])\n",
        "\n",
        "    # @staticmethod\n",
        "    # def build_resnet_152(input_shape, num_outputs):\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5XkKX59HCyl",
        "colab_type": "text"
      },
      "source": [
        "# Build the Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNijYHyBndLB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "# Step 2: Build network + define loss & solver\n",
        "#\n",
        "# retrieve dimensions of data for network construction\n",
        "img_rows, img_cols = 256, 256\n",
        "img_channels = 1\n",
        "num_classes = 5\n",
        "batch_size = 50\n",
        "epochs = 5\n",
        "\n",
        "\n",
        "model = ResnetBuilder.build_resnet_14b(input_shape=(img_rows, img_cols, img_channels), num_outputs=num_classes)\n",
        "\n",
        "#from keras.models import Sequential \n",
        "#model = Sequential()\n",
        "# Trying simple model first, to see if it works for .evaluate() on test datsemodel.add(Conv2D(64, (3, 3), input_shape=(1, 256, 256, padding='same',))\n",
        "#model.add(Conv2D(64, (1, 1), input_shape=(256, 256, 1)))\n",
        "#model.add(Dropout(0.25))\n",
        "#model.add(Flatten())\n",
        "#model.add(Dense(units=5, activation='softmax'))\n",
        "#model.summary()\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "print (\"Configured model for training\")\n",
        "\n",
        "# Create a callback Tensorboard object \n",
        "keras.callbacks.TensorBoard(log_dir='./Graph_Batch50_Epoch5', histogram_freq=0,  \n",
        "          write_graph=True, write_images=True)\n",
        "\n",
        "tbCallBack = keras.callbacks.TensorBoard(log_dir='./Graph_Batch50_Epoch5', histogram_freq=0, write_graph=True, write_images=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSno7rdrG8q8",
        "colab_type": "text"
      },
      "source": [
        "# Train!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2a_tFNQnk87",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "# Step 4: Train the model \n",
        "#\n",
        "# Fetch python array data from original data  \n",
        "\n",
        "# Train_IO\n",
        "train_data  = train_io.fetch_data('train_image').data()\n",
        "train_label = train_io.fetch_data('train_label').data()\n",
        "dim_data = train_io.fetch_data('train_image').dim()\n",
        "\n",
        "train_data_reshaped = train_data.reshape(dim_data)\n",
        "test_data  = test_io.fetch_data('test_image').data()\n",
        "test_label = test_io.fetch_data('test_label').data()\n",
        "test_data_reshaped = test_data.reshape(dim_data)\n",
        "\n",
        "\n",
        "history = model.fit(train_data_reshaped, train_label, \n",
        "\tbatch_size=batch_size, \n",
        "\tepochs=epochs, \n",
        "\tverbose=1, \n",
        "\t#validation_split=0.4\n",
        "    validation_data=(test_data_reshaped, test_label),\n",
        "    callbacks=[tbCallBack]\t# Gave your callback object to your function \n",
        "\t)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jl2JkUfnGo1d",
        "colab_type": "text"
      },
      "source": [
        "# Switching between Keras' training and inference modes\n",
        "\n",
        "The batch normalization layer is necessary after each convolutional layer. However, when run in Keras. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5FQSEUQGumI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"BEFORE SAVING MODEL\")\n",
        "K.set_learning_phase(0)\n",
        "scores = model.evaluate(test_data_reshaped, test_label, verbose=0)\n",
        "print('Test dataset loss:', scores[0])\n",
        "print('Test dataset accuracy:', scores[1])\n",
        "\n",
        "model.save('tmp.v1')\n",
        "\n",
        "# In every test we will clear the session and reload the model to force Learning_Phase values to change\n",
        "print('DYNAMIC LEARNING PHASE')\n",
        "K.clear_session()\n",
        "model = load_model('tmp.v1')\n",
        "scores = model.evaluate(test_data_reshaped, test_label, verbose=1)\n",
        "print('Test dataset loss:', scores[0])\n",
        "print('Test dataset accuracy:', scores[1])\n",
        "\n",
        "print('STATIC LEARNING PHASE = 0 TEST MODE')\n",
        "K.clear_session()\n",
        "model = load_model('tmp.v1')\n",
        "K.set_learning_phase(0)\n",
        "scores = model.evaluate(test_data_reshaped, test_label, verbose=1)\n",
        "print('Test dataset loss:', scores[0])\n",
        "print('Test dataset accuracy:', scores[1])\n",
        "\n",
        "print('STATIC LEARNING PHASE = 1 TRAIN MODE')\n",
        "K.clear_session()\n",
        "model = load_model('tmp.v1')\n",
        "K.set_learning_phase(1)\n",
        "\n",
        "scores = model.evaluate(test_data_reshaped, test_label, verbose=1)\n",
        "print('Test dataset loss:', scores[0])\n",
        "print('Test dataset accuracy:', scores[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pr3SoET-G_Lv",
        "colab_type": "text"
      },
      "source": [
        "# Plot Accuracies & Loss! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fkDL01ynoXY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot Accuracy & Loss\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_range = range(1, epochs+1)\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()\n",
        "# scores = model.evaluate(test_data_reshaped, test_label, verbose=0)\n",
        "# scores = model.evaluate(train_data_reshaped, train_label, verbose=0)\n",
        "\n",
        "#print('Test dataset loss:', scores[0])\n",
        "#print('Test dataset accuracy:', scores[1])\n",
        "\n",
        "#inform log directory\n",
        "print()\n",
        "print(\"You are done\") \n",
        "train_io.reset()\n",
        "test_io.reset()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}